For ease of notation, we define these shorter function names for calling numpy built ins. 

```python
import numpy as np
pi=np.pi
cos=np.cos
arctan=np.arctan
sqrt=np.sqrt
from scipy.integrate import quad
```

# Problem 1

The full code for our integrator and to generate the plots can be found in `problem1.py`. 

*One can work out the electric field from an infinitessimally thin spherical shell of charge with radius R by working out the field from a ring along its central axis, and integrating those rings to form a spherical shell.* 

The field from a ring of radius `r` will exert a force of `2*pi*cos(arctan(r/y))` at a distance `y` along it's axis. Therefore, to determine the electric field at distance `x` from the center of the sphere, we must integrate over `y` from `x-R` to `x+R`

```python
2*pi*sqrt(R**2-(x-y)**2)/(R**2+y**2)*cos(arctan(sqrt(R**2-(x-y)**2)/y))
```

For our custom integrator, I stole Jon's code and modified it so that it wouldn't crash when the max reccursion depth was encountered, setting a custom max depth of 10. 

```python
def integrate(fun,a,b,tol,rec_level=0):
    """Integrates fun over the interval [a,b] with adaptive 5-point integral"""
    x=np.linspace(a,b,5)
    dx=x[1]-x[0]
    y=fun(x)
    i1=(y[0]+4*y[2]+y[4])/3*(2*dx)
    i2=(y[0]+4*y[1]+2*y[2]+4*y[3]+y[4])/3*dx
    myerr=np.abs(i1-i2)
    if myerr<tol:
        return i2
    elif rec_level>=10:
        print("WARNING: recursion level 10 has been reached, stopping here.")
        print(f"WARNiNG: Error is still too large, myerr={myerr}")
        return return i2
    else:
        # Reccusion step
        mid=(a+b)/2
        int1=integrate(fun,a,mid,tol/2,rec_level+1)
        int2=integrate(fun,mid,b,tol/2,rec_level+1)
        return int1+int2
```

*Use both your integrator and scipy.integrate.quad to plot the electric field from the shell as a function of distance from the center of the sphere.*

Here's what the main body of this code looks like.

```python
"""main"""
# Set up the integral
R=1      #Radius of the sphere, (charge density is assumed to be 1)
tol=0.01 #Error Tolerance
xarr=np.arange(0,8,1/4) #Distances from center 0 to evaluate integral at
result_integrate=[] #Stores the values returned by our integrator
result_quad=[]      #Stores the values returned by scipy's integrator 
# Integrate over y for each distance that we care about
for x in xarr:
    def fun(y,R=R,x=x): 
        return 2*pi*sqrt(R**2-(x-y)**2)/(R**2+y**2)*cos(arctan(sqrt(R**2-(x-y)**2)/y))
    a,b=x-R,x+R
    print(f"DEBUG: x{x}, R{R}, a{a}, b{b}")
    #Apply Custom integrator
    result_integrate.append(integrate(fun,a,b,tol))
    #Apply Scipy's Fortran 'quadrature' integrator
    result_quad.append(quad(fun,a,b)[0])
```

*Make sure the range of your plot covers regions with z < R and z > R. Make sure one of your z values is R. Is there a singularity in the integral? Does quad care? Does your integrator? Note - if you get stuck setting up the problem, you may be able to find solutions to Griffiths problem 2.7, which sets up the integral.*

![nintegrate](https://user-images.githubusercontent.com/21654151/191822176-d42d655b-d0b4-4e7b-90ff-9dcb8ae7a3c0.png)


As you can see, the parameters defined above are such that `x<R` and `x>R` (I have renamed `z` to `x`). The following plot was generated by the code above. As you can see, there is a singularity at `x=R`. Quad doesn't care but our integrator does—the way we wrote ours, it returns nan values at the singularities, which is why there is no orange line on that little section of the code. 


# Problem 2

*Write a recursive variable step size integrator like the one we wrote in class that does NOT call f(x) multiple times for the same x. The function prototype should be*

```python
def integrate_adaptive(fun,a,b,tol,extra=None):
```

*where extra contains information a sub-call needs from preceeding calls. On the initial call, extra should be None, so the integrator knows it is starting off. For a few typical examples, how many function calls do you save vs the lazy way we wrote it in class?*

I modified the code, see below. Each subcall was doing 5 calls, now they only do 2. This is good, it's a linear time improvement of a factor of 5/2 (minus a tiny constant from the root node). If each function call costs one dollar and everything else is cheap, the amount of dollars we spend on average will be equal to the number of nodes we visit in our binary tree, times k, where k is either 5 or 2. Since the binary tree is equivalent, the time improvement is constant. However, if we increase the tolerance by an order of magnitude, we might expect the average depth of our binary tree to increase by one, therefore the time of evaluation will double. With this hand-wavey hunch, we estimate that for the same time cost, we get one extra degree of accuracy by going from k=5 to k=2 (since `2=5/2` roughly). 

```python
def integrate(fun,a,b,tol,extra=None,rec_level=0):
    """Integrates fun over the interval [a,b] with adaptive 5-point integral

    fun : function
        The function of one variable to integrate over
    a : float
        Lower bound
    b : float
        Upper bound
    tol : float
        Tolerance
    extra : np.ndarray
        An array of the three function evaluations that we have already made.
    rec_level : int
        The reccursion level. This is used to terminate the program without
        crashing if there is a singularity. The max reccursion level is hard
        coded to 10 for simplicity.
    """
    # At the first reccursion level, extra is None, sample 5 points
    x=np.linspace(a,b,5)
    dx=x[1]-x[0]
    if extra is None:
        y=fun(x) #Evaluate at all 5 points
    else:
        y=np.empty(5) # Init y array to be empty
        y[::2]=extra  # Fill out what we already know about y
        y[1::2]=fun(x[1::2]) # Only evaluate points we haven't already evaluated
    i1=(y[0]+4*y[2]+y[4])/3*(2*dx) # Simpson's rule, 3point integral
    i2=(y[0]+4*y[1]+2*y[2]+4*y[3]+y[4])/3*dx # 5point integral
    myerr=np.abs(i1-i2)
    if myerr<tol:
        return i2
    elif rec_level>=10:
        print("WARNING: recursion level 10 has been reached, stopping here.")
        print(f"WARNiNG: Error is still too large, myerr={myerr}")
        return i2
    else:
        # Reccusion step
        mid=(a+b)/2
        # Extra is the first three steps (first half)
        int1=integrate(fun,a,mid,tol/2,y[:3],rec_level+1) 
        # Extra is the final three steps (latter half) (there is overlap on purpose!)
        int2=integrate(fun,mid,b,tol/2,y[2:],rec_level+1)
        return int1+int2
```

The above code was tested and it generates the same plots as in problem 1. To see this for yourself, execute `python problem2.py`.

# Problem 3

## a)

*Write a function that models the log base 2 of x valid from 0.5 to 1 to an accuracy in the region better than 10−6. Please use a truncated Chebyshev polynomial fit to do this - you can use np.polynomial.chebyshev.chebfit. How many terms do you need? You should use many x/y values and fit to some high order, then only keep the terms you think you’ll need and drop the rest. Make sure also that you rescale the x-range you use to go from -1 to 1 before calling chebfit.*

```python
# Perpare data to fit
x=np.linspace(0.5,1,1001)
y=log2(x)
xscale=x*4-3 # Scale x between -1 and 1
# Fit a chebyshev polynomial
degree=7
cheb_coeffs=chebfit(xscale,y,deg=degree)
# Scale the fit so that it's in the right range
def fastlog2(x):
    return chebval(x*4-3,cheb_coeffs)
# Make sure the error is okay by plotting residuals
xfine=np.linspace(0.5,1,10001)
res=fastlog2(xfine)-log2(xfine)
```

*Once you have the Chebyshev expansion for 0.5 to 1, write a routine called
mylog2 that will take the natural log of any positive number. Hint - you will want to use the routine np.frexp for this, which breaks up a floating point number into its mantissa and exponent. Also feel free to use np.polynomial.chebyshev.chebval to evaluate your fit. You might ask yourself if a computer ever takes a natural log directly, or if it goes through the log base 2 (near as I can tell, it’s the log base 2).*

The natural log of a number `ln(x)` is just `ln(2)*log2(x)`. To compute the log base two of `x`, we split it into it's mantissa and it's exponant, then sum the log of it's mantissa with it's exponant. The Mantissa will always be between 0.5 and 1, so we can use our cheb fit!

```python
"""Part a), get the log of any positive number."""
def mylog2(x):
    mantissa,exponant=np.frexp(x)
    # For positive numbers, the mantissa will never be less than 0.5
    return fastlog2(mantissa) + exponant

def myln(x):
    return np.log(2) * mylog2(x)
```
This agrees quite well with numpy log. Residuals:










